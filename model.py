import keras
import numpy as np
from keras import Model
from keras import layers
from keras.layers import Dropout, BatchNormalization, Embedding, SpatialDropout1D, GlobalMaxPooling1D, Conv1D, \
    concatenate, Dense, Activation, MaxPool1D, Flatten, Lambda
from keras.utils import plot_model


def get_model(shape=(500, 64), num_classes=2, input_dim=93, model_type=0, **kwargs):
    if model_type == 4:
        return deeper_textcnn(shape, num_classes, input_dim, **kwargs)
    elif model_type == 6:
        return cnn_autoencoder(shape, num_classes, input_dim, **kwargs)
    else:
        print("error")


def deeper_textcnn(shape=(500, 64), num_classes=2, input_dim=93, n=1, use_attention=False, **kwargs):
    input_array = keras.Input(shape=(shape[0],), name='input')

    embedding = Embedding(input_dim=input_dim, output_dim=shape[1])
    embedding_output = embedding(input_array)

    _embed = SpatialDropout1D(0.25)(embedding_output)
    warppers = []
    kernel_size = [2, 3, 4, 5]

    for _kernel_size in kernel_size:
        for dilated_rate in [1, 2, 3, 4]:
            num_res_blocks = 1
            num_filters_in = 64
            conv1d = Conv1D(filters=num_filters_in, kernel_size=_kernel_size, dilation_rate=dilated_rate)(_embed)
            b = BatchNormalization()(conv1d)
            r = Activation("elu")(b)
            x = r

            conv1 = Conv1D(filters=num_filters_in, kernel_size=1, padding="same")(x)
            b = BatchNormalization()(conv1)
            r = Activation("elu")(b)
            conv2 = Conv1D(filters=num_filters_in, kernel_size=3, padding="same")(r)
            b = BatchNormalization()(conv2)
            r = Activation("elu")(b)

            x = keras.layers.add([x, r])

            x = BatchNormalization()(x)
            x = Activation('elu')(x)
            warppers.append(GlobalMaxPooling1D()(x))

    fc = concatenate(warppers)
    fc = Dropout(0.25)(fc)
    fc = Dense(50, activation='relu', name="feature_layer")(fc)
    preds = Dense(num_classes, activation='softmax')(fc)

    model = Model(inputs=input_array, outputs=preds)

    model.compile(loss=l2_softmax(5),
                  optimizer='adam',
                  metrics=['accuracy'])
    model.summary()
    plot_model(model, "attention.png")
    return model


def cnn_autoencoder(shape=(500, 64), num_classes=2, input_dim=93, n=1, use_attention=False, **kwargs):
    input_array = keras.Input(shape=(shape[0],), name='input')

    embedding = Embedding(input_dim=input_dim, output_dim=shape[1], weights=[np.load("weight.npy")])
    embedding.trainable = False
    embedding_output = embedding(input_array)

    _embed = SpatialDropout1D(0.25)(embedding_output)
    kernel_size = 3
    dilated_rate = 1

    num_filters_in = 64
    conv1d = Conv1D(filters=num_filters_in, kernel_size=kernel_size, padding="same")(_embed)
    b = BatchNormalization()(conv1d)
    r = Activation("elu")(b)
    x = r

    conv1 = Conv1D(filters=num_filters_in, kernel_size=1, padding="same")(x)
    b = BatchNormalization()(conv1)
    r = Activation("elu")(b)
    conv2 = Conv1D(filters=num_filters_in, kernel_size=3, padding="same")(r)
    b = BatchNormalization()(conv2)
    r = Activation("elu")(b)

    x = keras.layers.add([x, r])

    x = BatchNormalization()(x)
    x = Activation('elu')(x)
    x = MaxPool1D(pool_size=128, strides=128, data_format='channels_last')(x)
    fc = Activation("sigmoid", name="feature_output")(x)

    fc = layers.UpSampling1D(size=128)(fc)
    fc = BatchNormalization()(fc)
    fc = Activation('elu')(fc)  # add later
    # mid = Flatten(name="feature_output")(fc)

    conv2 = Conv1D(filters=num_filters_in, kernel_size=3, padding="same")(fc)
    b = BatchNormalization()(conv2)
    r = Activation("elu")(b)

    conv1 = Conv1D(filters=num_filters_in, kernel_size=1, padding="same")(r)
    b = BatchNormalization()(conv1)
    r = Activation("elu")(b)

    x = keras.layers.add([fc, r])
    conv1d = Conv1D(filters=num_filters_in, kernel_size=kernel_size, padding="same")(x)
    b = BatchNormalization()(conv1d)
    r = Activation("elu")(b)

    output = r

    output = Lambda(lambda x: keras.losses.mean_squared_error(x[0], x[1]), name='loss',
                    output_shape=(1,))([output, embedding_output])

    model = Model(inputs=input_array, outputs=output)

    model.compile(loss=loss_first,
                  optimizer='adam')
    model.summary()
    plot_model(model, "attention.png")
    return model


def loss_first(x, y):
    return y


def l2_softmax(alpha):
    def a(y_true, y_pred):
        y_normal = alpha * keras.backend.l2_normalize(y_pred)

        return keras.losses.categorical_crossentropy(y_true, y_normal)

    return a


def load_model(model_path, load_type=0) -> keras.Model:
    """
    返回训练好的模型
    :return:
    """

    def temp(a, b=0):
        return a

    if load_type == 0:
        a = temp
        return keras.models.load_model(model_path, custom_objects={'a': l2_softmax(5)})
    elif load_type == 1:
        a = temp
        model = keras.models.load_model(model_path, custom_objects={'a': l2_softmax(5)})
        plot_model(model, "attention.png")

        output = model.get_layer('feature_layer').output
        model = Model(model.input, output)
        model.summary()
        return model
    elif load_type == 2:
        a = temp
        model = keras.models.load_model(model_path,
                                        custom_objects={'a': l2_softmax(5), "keras": keras, "loss_first": loss_first})
        plot_model(model, "attention.png")

        output = model.get_layer('feature_output').output
        output = Flatten()(output)
        model = Model(model.input, output)
        model.summary()
        return model


if __name__ == '__main__':
    model = load_model("weights.01-0.06816901.hdf5", 1)
    plot_model(model, "attention.png")
