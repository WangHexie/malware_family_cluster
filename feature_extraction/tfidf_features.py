import os

import pandas as pd
import scipy
from sklearn.feature_extraction.text import TfidfVectorizer

from basic_function import load_df, save_dict, get_root_path


def to_str(df, mode=0, column_name=None):
    if column_name is None:
        column_name = 'api_name'
    string_list = []
    name_list = []
    for i in df.groupby('file_name')[column_name]:
        name_list.append(i[0])
        api_str = ""
        for p in i[1].iteritems():
            api_str += " " + str(p[1])
        string_list.append(api_str)
    # print(name_list)
    if mode == 1:
        return string_list, name_list
    else:
        return string_list


def api():
    api_vec = TfidfVectorizer(ngram_range=(1, 5),
                              min_df=3, max_df=0.9,
                              strip_accents='unicode',
                              use_idf=1, smooth_idf=1, sublinear_tf=1, max_features=500)

    white = pd.read_csv("white.csv")[['file_name', 'api_name']]
    black = load_df("black")
    test = load_df("test")

    full = pd.concat([white, black, test])
    full_str = to_str(full)

    print(1)
    api_vec.fit(full_str)

    print(2)
    black_output, name_list = to_str(black, mode=1)
    save_dict(name_list, "black_name_list")
    black_output = api_vec.transform(black_output)
    scipy.sparse.save_npz("black.npz", black_output)

    white_output, name_list = to_str(white, mode=1)
    save_dict(name_list, "white_name_list")
    white_output = api_vec.transform(white_output)
    scipy.sparse.save_npz("white.npz", white_output)

    test_str, name_list = to_str(test, mode=1)
    save_dict(name_list, "test_name_list")
    test_output = api_vec.transform(test_str)
    scipy.sparse.save_npz("test.npz", test_output)


def stage2_api(feature_num=500):
    api_vec = TfidfVectorizer(ngram_range=(1, 5),
                              min_df=3, max_df=0.9,
                              strip_accents='unicode',
                              use_idf=1, smooth_idf=1, sublinear_tf=1, max_features=feature_num)

    white = pd.read_csv(os.path.join("features", "white.csv"))[['file_name', 'api_name']]
    black = load_df(os.path.join("features", "black"))
    test = load_df(os.path.join("features", "test"))
    stage2 = load_df(os.path.join("features", "stage2"))

    full = pd.concat([white, black, test])
    full_str = to_str(full)

    print(1)
    api_vec.fit(full_str)

    print(2)

    black_output, name_list = to_str(stage2, mode=1)
    save_dict(name_list, os.path.join("features", "stage2_name_list"+str(feature_num)))
    black_output = api_vec.transform(black_output)
    scipy.sparse.save_npz(os.path.join("features", "stage2"+str(feature_num)+".npz"), black_output)


def tianchi_api():
    api_vec = TfidfVectorizer(ngram_range=(1, 5),
                              min_df=3, max_df=0.9,
                              strip_accents='unicode',
                              use_idf=1, smooth_idf=1, sublinear_tf=1, max_features=500)

    white = pd.read_csv(os.path.join("features", "white.csv"))[['file_name', 'api_name']]
    black = load_df(os.path.join("features", "black"))
    test = load_df(os.path.join("features", "test"))
    tianchi = pd.read_csv("security_train.csv").rename(columns={"file_id":"file_name"})

    full = pd.concat([white, black, test])
    full_str = to_str(full)

    print(1)
    api_vec.fit(full_str)

    print(2)

    black_output, name_list = to_str(tianchi, mode=1, column_name="api")
    save_dict(name_list, os.path.join("features", "tianchi_name_list"))
    black_output = api_vec.transform(black_output)
    scipy.sparse.save_npz(os.path.join("features", "tianchi.npz"), black_output)


def stage_2_attribute(suffix="_dll", use_less_value=False, type_name="", map_func=None, max_feature=1000):
    stage2 = load_df(os.path.join("features", "stage2"+suffix), mode=1)

    if use_less_value:
        if map_func is None:
            stage2["value"] = stage2["value"].map(lambda x: x.split("\\")[-1])
        else:
            stage2["value"] = stage2["value"].map(lambda x: map_func(x))
    stage2_output, name_list = to_str(stage2, mode=1, column_name="value")
    api_vec, _ = train_tf_idf(suffix="_dll", use_less_value=use_less_value, map_func=map_func, data=stage2_output, max_feature=max_feature)

    save_dict(name_list, os.path.join(get_root_path(), "features", "stage2_name_list" + suffix + type_name))
    stage2_output = api_vec.transform(stage2_output)
    scipy.sparse.save_npz(os.path.join(get_root_path(), "features", "stage2" + suffix + type_name + ".npz"), stage2_output)


def attribution(suffix="_dll", use_less_value=False, type_name="", map_func=None, max_feature=2000):
    api_vec, data = train_tf_idf(suffix="_dll", use_less_value=use_less_value, map_func=map_func,
                                 max_feature=max_feature)

    white, black, test = data

    black_output, name_list = to_str(black, mode=1, column_name="value")
    save_dict(name_list, os.path.join(get_root_path(), "features", "black_name_list" + suffix + type_name))
    black_output = api_vec.transform(black_output)
    scipy.sparse.save_npz(os.path.join(get_root_path(), "features", "black" + suffix + type_name + ".npz"), black_output)

    white_output, name_list = to_str(white, mode=1, column_name="value")
    save_dict(name_list, os.path.join(get_root_path(), "features", "white_name_list" + suffix + type_name))
    white_output = api_vec.transform(white_output)
    scipy.sparse.save_npz(os.path.join(get_root_path(), "features", "white" + suffix + type_name + ".npz"), white_output)

    test_str, name_list = to_str(test, mode=1, column_name="value")
    save_dict(name_list, os.path.join(get_root_path(), "features", "test_name_list" + suffix + type_name))
    test_output = api_vec.transform(test_str)
    scipy.sparse.save_npz(os.path.join(get_root_path(), "features", "test" + suffix + type_name + ".npz"), test_output)


def train_tf_idf(suffix="_dll", use_less_value=False, map_func=None, max_feature=2000, data=None):
    api_vec = TfidfVectorizer(ngram_range=(1, 5),
                              min_df=3, max_df=0.9,
                              strip_accents='unicode',
                              use_idf=1, smooth_idf=1, sublinear_tf=1, max_features=max_feature)

    if data is None:
        white = load_df(os.path.join(get_root_path(), "features", "white" + suffix), mode=1)
        black = load_df(os.path.join(get_root_path(), "features", "black" + suffix), mode=1)
        test = load_df(os.path.join(get_root_path(), "features", "test" + suffix), mode=1)

        if use_less_value:
            if map_func is None:
                for i in [white, black, test]:
                    i["value"] = i["value"].map(lambda x: x.split("\\")[-1])
            else:
                for i in [white, black, test]:
                    i["value"] = i["value"].map(lambda x: map_func(x))

        full = pd.concat([white, black, test])
        full_str = to_str(full, column_name="value")
    else:
        full_str = data

    print(1)
    api_vec.fit(full_str)
    print(2)
    if data is None:
        return api_vec, [white, black, test]
    else:
        return api_vec, None


def last_hkey(x):
    return "speech" if len(x.split("\\")) == 1 else x.split("\\")[-1]


def second_hkey(x):
    return "speech" if len(x.split("\\")) == 1 else x.split("\\")[1]


def stage2_api_new(feature_num=500):
    api_vec = TfidfVectorizer(ngram_range=(1, 5),
                              min_df=3, max_df=0.9,
                              strip_accents='unicode',
                              use_idf=1, smooth_idf=1, sublinear_tf=1, max_features=feature_num)


    stage2 = load_df(os.path.join("features", "stage2"))

    black_output, name_list = to_str(stage2, mode=1)

    print(1)
    api_vec.fit(black_output)

    print(2)

    # black_output, name_list = to_str(stage2, mode=1)
    save_dict(name_list, os.path.join(get_root_path(), "features", "stage2_name_list"+str(feature_num)))
    black_output = api_vec.transform(black_output)
    scipy.sparse.save_npz(os.path.join(get_root_path(), "features", "stage2"+str(feature_num)+".npz"), black_output)

# attribution("_hkey", use_less_value=True, type_name="second", map_func=second_hkey, max_feature=100)
# attribution("_hkey", use_less_value=True, type_name="last", map_func=last_hkey, max_feature=200)
# attribution("_hkey", use_less_value=False, type_name="", max_feature=500)
# attribution("_dll", use_less_value=False, type_name="", max_feature=500)
# tianchi_api()
# api()


stage_2_attribute("_dll", False, max_feature=1000)
stage_2_attribute("_hkey", use_less_value=True, type_name="last", map_func=last_hkey, max_feature=1000)
stage_2_attribute("_hkey", use_less_value=True, type_name="first", map_func=second_hkey, max_feature=100)
stage2_api_new(1000)
